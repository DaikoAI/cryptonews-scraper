"""
CryptoPanic News Scraper

ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
data_sourceãƒ†ãƒ¼ãƒ–ãƒ«å¯¾å¿œç‰ˆ - åŠ¹ç‡çš„ãªå·®åˆ†æ›´æ–°
"""

import json
import os
from datetime import datetime

from src.models import DataSource
from src.scrapers import CryptoPanicScraper
from src.storage import PostgresStorage
from src.utils.logger import get_app_logger
from src.webdriver import create_webdriver_from_env


async def check_for_updates(storage: PostgresStorage) -> datetime | None:
    """å‰å›å–å¾—æ™‚åˆ»ã‚’ãƒã‚§ãƒƒã‚¯ - åŠ¹ç‡çš„ãªå·®åˆ†åˆ¤å®šç”¨"""
    try:
        last_published = await storage.get_latest_published_at()
        logger = get_app_logger(__name__)

        if last_published:
            logger.info(f"âœ… Last article published: {last_published}")
        else:
            logger.info("ğŸ†• No previous articles found - first run")

        return last_published
    except Exception as e:
        logger = get_app_logger(__name__)
        logger.warning(f"âš ï¸ Could not check last update: {e}")
        return None


async def setup_storage() -> PostgresStorage | None:
    """PostgreSQLæ¥ç¶šã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
    logger = get_app_logger(__name__)

    try:
        storage = PostgresStorage()
        await storage.connect()
        logger.info("ğŸ“Š PostgreSQL connected successfully")
        return storage
    except Exception as e:
        logger.warning(f"âŒ PostgreSQL connection failed: {e}")
        logger.info("ğŸ’¾ Will fallback to JSON file storage")
        return None


def filter_new_articles(data_sources: list[DataSource], last_published: datetime | None) -> list[DataSource]:
    """æ–°ã—ã„è¨˜äº‹ã®ã¿ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚° - åŠ¹ç‡çš„ãªå·®åˆ†æŠ½å‡º"""
    if not last_published:
        return data_sources

    new_articles = []
    for ds in data_sources:
        # å…¬é–‹æ—¥æ™‚ãŒå–å¾—ã§ããªã„å ´åˆã¯æ–°ã—ã„ã‚‚ã®ã¨ã—ã¦æ‰±ã†
        if not ds.published_at or ds.published_at > last_published:
            new_articles.append(ds)

    return new_articles


async def save_to_fallback_json(data_sources: list[DataSource]) -> None:
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä¿å­˜"""
    logger = get_app_logger(__name__)

    json_data = {
        "scraped_at": datetime.now().isoformat(),
        "count": len(data_sources),
        "data_sources": [ds.to_dict() for ds in data_sources],
    }

    os.makedirs("reports", exist_ok=True)
    json_file = f"reports/crypto_news_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"

    with open(json_file, "w", encoding="utf-8") as f:
        json.dump(json_data, f, ensure_ascii=False, indent=2, default=str)

    logger.info(f"ğŸ’¾ Saved {len(data_sources)} articles to {json_file}")


async def run_scraping() -> None:
    """ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç† - åŠ¹ç‡çš„ãªå·®åˆ†æ›´æ–°"""
    logger = get_app_logger(__name__)
    logger.info("ğŸš€ Starting efficient CryptoPanic scraping...")

    # 1. PostgreSQLæ¥ç¶šç¢ºèª
    storage = await setup_storage()
    last_published_at = None

    if storage:
        last_published_at = await check_for_updates(storage)

    # 2. WebDriverã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
    with create_webdriver_from_env() as driver:
        scraper = CryptoPanicScraper(driver)
        all_data_sources = scraper.run_scraping()

    logger.info(f"ğŸ“„ Scraped {len(all_data_sources)} total articles")

    # 3. æ–°ã—ã„è¨˜äº‹ã®ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ï¼ˆåŠ¹ç‡åŒ–ãƒã‚¤ãƒ³ãƒˆï¼‰
    new_data_sources = filter_new_articles(all_data_sources, last_published_at)

    # 4. æ—©æœŸçµ‚äº†: æ–°ã—ã„è¨˜äº‹ãŒãªã„å ´åˆï¼ˆCPUç¯€ç´„ï¼‰
    if not new_data_sources:
        logger.info("âš¡ No new articles - terminating early to save CPU")
        if storage:
            await storage.close()
        return

    logger.info(f"ğŸ†• Found {len(new_data_sources)} new articles")

    # 5. ãƒ‡ãƒ¼ã‚¿ä¿å­˜
    saved_successfully = False

    if storage:
        try:
            saved_count = await storage.save_data_sources(new_data_sources)
            logger.info(f"âœ… Saved {saved_count} articles to PostgreSQL")
            saved_successfully = True
        except Exception as e:
            logger.error(f"âŒ PostgreSQL save failed: {e}")
        finally:
            await storage.close()

    # 6. ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä¿å­˜
    if not saved_successfully:
        await save_to_fallback_json(new_data_sources)


async def main():
    """ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    logger = get_app_logger(__name__)

    try:
        await run_scraping()
        logger.info("âœ… Scraping completed successfully")

    except Exception as e:
        logger.error(f"ğŸ’¥ Scraping failed: {e}")
        raise


if __name__ == "__main__":
    import asyncio

    asyncio.run(main())
