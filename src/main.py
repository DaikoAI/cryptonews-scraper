"""
CryptoPanic News Scraper

ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
åŠ¹ç‡çš„ãªå·®åˆ†æ›´æ–° - æ–°è¦è¨˜äº‹ãŒã‚ã‚‹å ´åˆã®ã¿ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
"""

from datetime import datetime

from src.scrapers import CryptoPanicScraper
from src.storage import PostgresStorage
from src.utils.logger import get_app_logger
from src.webdriver import create_webdriver_from_env


async def get_last_published_timestamp(storage: PostgresStorage) -> datetime | None:
    """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‹ã‚‰å‰å›å–å¾—ã—ãŸæœ€æ–°è¨˜äº‹ã®å…¬é–‹æ—¥æ™‚ã‚’å–å¾—"""
    try:
        logger = get_app_logger(__name__)
        logger.info("ğŸ“… Checking last published timestamp...")

        last_published = await storage.get_latest_published_at()

        if last_published:
            logger.info(f"âœ… Found previous latest article: {last_published}")
        else:
            logger.info("ğŸ†• First run - no previous articles found")

        return last_published
    except Exception as e:
        logger = get_app_logger(__name__)
        logger.error(f"âŒ Failed to get last published timestamp: {e}")
        return None


async def connect_to_database() -> PostgresStorage | None:
    """PostgreSQLãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«æ¥ç¶š"""
    logger = get_app_logger(__name__)
    logger.info("ğŸ”Œ Connecting to database...")

    try:
        storage = PostgresStorage()
        await storage.connect()
        logger.info("âœ… Database connected successfully")
        return storage
    except Exception as e:
        logger.error(f"âŒ Database connection failed: {e}")
        return None


async def scrape_and_save_new_articles() -> None:
    """æ–°è¦è¨˜äº‹ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã¨ä¿å­˜"""
    logger = get_app_logger(__name__)
    logger.info("ğŸš€ Starting CryptoPanic news scraper...")

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
    storage = await connect_to_database()
    last_published_at = None

    if storage:
        last_published_at = await get_last_published_timestamp(storage)

    # News elementå–å¾—ã¨ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    with create_webdriver_from_env() as driver:
        scraper = CryptoPanicScraper(driver)
        filtered_elements = scraper.get_filtered_elements_by_date(last_published_at)

        if not filtered_elements:
            logger.info("âš¡ No new articles found - process completed")
            if storage:
                await storage.close()
            return

        logger.info(f"ğŸ“° Found {len(filtered_elements)} new articles - starting scraping")
        scraped_articles = scraper.scrape_filtered_articles(filtered_elements)

    logger.info(f"ğŸ“„ Scraped {len(scraped_articles)} articles")

    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜
    if storage and scraped_articles:
        try:
            logger.info(f"ğŸ’¾ Saving {len(scraped_articles)} articles to database...")
            saved_count = await storage.save_data_sources(scraped_articles)

            # ä¿å­˜çµæœã®è©³ç´°ã‚µãƒãƒªãƒ¼
            if saved_count == len(scraped_articles):
                logger.info(f"âœ… Successfully saved all {saved_count} articles to database")
            elif saved_count > 0:
                skipped_count = len(scraped_articles) - saved_count
                logger.warning(
                    f"âš ï¸ Saved {saved_count}/{len(scraped_articles)} articles ({skipped_count} skipped due to duplicates/validation errors)"
                )
            else:
                logger.warning(f"âš ï¸ No new articles saved (all {len(scraped_articles)} were duplicates or invalid)")

        except Exception as e:
            logger.error(f"âŒ Database save failed: {e}")
        finally:
            await storage.close()
    else:
        if not storage:
            logger.warning("âš ï¸ No database connection - articles not saved")
        elif not scraped_articles:
            logger.warning("âš ï¸ No articles to save")

    logger.info("ğŸ‰ Process completed successfully")


async def main():
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    logger = get_app_logger(__name__)

    try:
        await scrape_and_save_new_articles()

    except KeyboardInterrupt:
        logger.info("â¹ï¸ Process interrupted by user")

    except Exception as e:
        logger.error(f"ğŸ’¥ Unexpected error occurred: {e}")
        raise


if __name__ == "__main__":
    import asyncio

    asyncio.run(main())
