"""
CryptoPanic News Scraper

CryptoPanicã‚µã‚¤ãƒˆã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã™ã‚‹ã‚¯ãƒ©ã‚¹
"""

import re
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import UTC, datetime
from urllib.parse import urljoin

from selenium.common.exceptions import TimeoutException
from selenium.webdriver.common.by import By
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait

from src.config import Config
from src.models import DataSource
from src.scrapers.base import BaseScraper
from src.utils.scraping_utils import (
    ElementSearcher,
    TabManager,
    TextCleaner,
    WebDriverUtils,
)


class CryptoPanicScraper(BaseScraper):
    """CryptoPanicå°‚ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ - é«˜é€Ÿä¸¦åˆ—ç‰ˆ"""

    BASE_URL = "https://cryptopanic.com"

    def __init__(self, driver):
        super().__init__(driver)
        self.tab_manager = TabManager(driver.driver, self.logger)

        # URLè¨­å®š
        self.url = f"{self.BASE_URL}/"

        # ä¸¦åˆ—å‡¦ç†è¨­å®š
        self.max_workers = Config.SCRAPING_MAX_WORKERS
        self.batch_size = Config.SCRAPING_BATCH_SIZE
        self.driver_lock = threading.Lock()  # WebDriveræ“ä½œã®ãƒ­ãƒƒã‚¯

        self.logger.info(f"ğŸš€ Parallel processing: {self.max_workers} workers, batch size: {self.batch_size}")

    def get_source_name(self) -> str:
        return "cryptopanic"

    def get_base_url(self) -> str:
        """ãƒ™ãƒ¼ã‚¹URLã‚’å–å¾—"""
        return self.url

    def scrape_articles(self) -> list[DataSource]:
        """å…¨è¨˜äº‹ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆBaseScraperç”¨ï¼‰"""
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ãªã—ã§å…¨è¨˜äº‹ã‚’å–å¾—
        all_elements = self.get_filtered_elements_by_date(None)
        return self.scrape_filtered_articles(all_elements)

    def get_filtered_elements_by_date(self, last_published_at: datetime | None) -> list:
        """æ—¥æ™‚ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ã®elementãƒªã‚¹ãƒˆã‚’å–å¾—"""
        self.logger.info("ğŸ” Loading news page and filtering articles...")

        # ãƒšãƒ¼ã‚¸èª­ã¿è¾¼ã¿
        self.driver.driver.get(self.url)
        self.driver.driver.set_window_size(1920, 1080)

        # å…¨è¦ç´ ã®å–å¾—
        self._scroll_to_load_all_elements()
        all_elements = self._get_article_elements()

        # æ—¥æ™‚ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        filtered_elements = self._filter_elements_by_published_date(all_elements, last_published_at)

        self.logger.info(f"ğŸ“Š Found {len(filtered_elements)}/{len(all_elements)} new articles")

        return filtered_elements

    def _filter_elements_by_published_date(self, elements: list, last_published_at: datetime | None) -> list:
        """elementã‚’å…¬é–‹æ—¥æ™‚ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        if not last_published_at:
            self.logger.info("No last_published_at provided, returning all elements")
            return elements

        self.logger.info(f"ğŸ•°ï¸ Filtering articles newer than: {last_published_at}")

        filtered_elements = []
        same_count = 0
        older_count = 0
        newer_count = 0
        invalid_count = 0

        for i, element in enumerate(elements):
            try:
                published_at = self._extract_published_at(element)

                # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼ˆæœ€åˆã®5å€‹ã®ã¿ï¼‰
                if i < 5:
                    title_preview = self._extract_title_from_element(element)
                    title_preview = (
                        title_preview[:30] + "..." if title_preview and len(title_preview) > 30 else title_preview
                    )
                    self.logger.debug(f"Element {i + 1}: '{title_preview}' -> published_at={published_at}")

                # æ™‚é–“æƒ…å ±ãŒå–å¾—ã§ããªã„è¨˜äº‹ã¯æ–°ã—ã„è¨˜äº‹ã¨ã—ã¦æ‰±ã†ï¼ˆå®‰å…¨ã‚µã‚¤ãƒ‰ï¼‰
                if not published_at:
                    filtered_elements.append(element)
                    invalid_count += 1
                    if i < 5:
                        self.logger.debug(f"Element {i + 1}: No published_at, treating as new article")
                elif published_at > last_published_at:
                    filtered_elements.append(element)
                    newer_count += 1
                elif published_at == last_published_at:
                    same_count += 1
                    if i < 5:
                        self.logger.debug(f"Element {i + 1}: Same time as last article, skipping")
                else:
                    older_count += 1
                    if i < 5:
                        self.logger.debug(f"Element {i + 1}: Older than last article, skipping")

            except Exception as e:
                # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã‚‚æ–°ã—ã„è¨˜äº‹ã¨ã—ã¦æ‰±ã†ï¼ˆå®‰å…¨ã‚µã‚¤ãƒ‰ï¼‰
                filtered_elements.append(element)
                invalid_count += 1
                self.logger.debug(f"Failed to parse date for element {i + 1}: {e}, treating as new article")
                continue

        self.logger.info(
            f"ğŸ“Š Filtering results: {newer_count} newer, {same_count} same, {older_count} older, {invalid_count} no-time (treated as new)"
        )

        # æ–°ã—ã„é †ã«ã‚½ãƒ¼ãƒˆï¼ˆã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³å•é¡Œã‚’è§£æ±ºï¼‰
        filtered_elements.sort(
            key=lambda x: self._extract_published_at(x) or datetime.min.replace(tzinfo=UTC), reverse=True
        )

        return filtered_elements

    def scrape_filtered_articles(self, filtered_elements: list) -> list[DataSource]:
        """ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ã®elementã®ã¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        if not filtered_elements:
            return []

        self.logger.info(f"ğŸ”„ Scraping {len(filtered_elements)} articles...")

        result = self._process_article_elements(filtered_elements)

        self.logger.info(f"âœ… Successfully scraped {len(result)} articles")

        return result

    def _scroll_to_load_all_elements(self) -> None:
        """ãƒšãƒ¼ã‚¸ã‚’æœ€ä¸‹éƒ¨ã¾ã§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦å…¨è¦ç´ ã‚’ãƒ­ãƒ¼ãƒ‰"""
        try:
            self.logger.info("   ğŸ”„ Scrolling to load all elements...")

            last_height = self.driver.driver.execute_script("return document.body.scrollHeight")
            scroll_attempts = 0
            max_attempts = 10

            while scroll_attempts < max_attempts:
                # ãƒšãƒ¼ã‚¸æœ€ä¸‹éƒ¨ã¾ã§ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«
                self.driver.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")

                # å‹•çš„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒ­ãƒ¼ãƒ‰ã‚’å¾…æ©Ÿ
                time.sleep(2)

                # æ–°ã—ã„ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸã‹ãƒã‚§ãƒƒã‚¯
                new_height = self.driver.driver.execute_script("return document.body.scrollHeight")
                if new_height == last_height:
                    break

                last_height = new_height
                scroll_attempts += 1
                self.logger.debug(f"   ğŸ“ Scroll attempt {scroll_attempts}/{max_attempts}, height: {new_height}")

            # æœ€å¾Œã«ãƒšãƒ¼ã‚¸ãƒˆãƒƒãƒ—ã«æˆ»ã‚‹
            self.driver.driver.execute_script("window.scrollTo(0, 0);")
            time.sleep(1)

            self.logger.info(f"   âœ… Scrolling completed after {scroll_attempts} attempts")

        except Exception as e:
            self.logger.warning(f"   âš ï¸ Scrolling failed: {e}")

    def _process_article_elements(self, elements: list) -> list[DataSource]:
        """è¨˜äº‹è¦ç´ ã‚’ä¸¦åˆ—å‡¦ç†ã§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        if not elements:
            return []

        max_workers = self.max_workers
        batch_size = self.batch_size

        data_sources = []
        total_count = len(elements)

        # ãƒãƒƒãƒå‡¦ç†
        for i in range(0, total_count, batch_size):
            batch_elements = elements[i : i + batch_size]
            batch_results = self._process_elements_batch(batch_elements, max_workers)

            # çµæœã‚’ãƒãƒ¼ã‚¸
            for result in batch_results:
                if result["success"] and result["data_source"]:
                    data_sources.append(result["data_source"])

        return data_sources

    def _process_elements_batch(self, elements: list, max_workers: int) -> list[dict]:
        """è¦ç´ ã®ãƒãƒƒãƒã‚’ä¸¦åˆ—å‡¦ç†"""
        results = []

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # å„elementã‚’ä¸¦åˆ—ã§å‡¦ç†
            future_to_element = {
                executor.submit(self._process_single_element, element): element for element in elements
            }

            # å®Œäº†é †ã«çµæœã‚’åé›†
            for future in as_completed(future_to_element):
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    self.logger.warning(f"Element processing failed: {e}")
                    results.append({"success": False, "data_source": None, "error": str(e)})

        return results

    def _process_single_element(self, element) -> dict:
        """å˜ä¸€elementã‚’å®‰å…¨ã«å‡¦ç†"""
        try:
            # WebDriverã‚¢ã‚¯ã‚»ã‚¹ã¯æ…é‡ã«ãƒ­ãƒƒã‚¯
            with self.driver_lock:
                # åŸºæœ¬ãƒã‚§ãƒƒã‚¯
                try:
                    _ = element.tag_name  # stale check
                    if not element.is_displayed():
                        return {
                            "success": False,
                            "data_source": None,
                            "error": "Element not displayed",
                        }
                except Exception as e:
                    return {
                        "success": False,
                        "data_source": None,
                        "error": f"Element stale: {e}",
                    }

                # ã‚¿ã‚¤ãƒˆãƒ«ã¨CryptoPanic URLã‚’å–å¾—
                title = self._extract_title_from_element(element)
                cryptopanic_url = self._extract_cryptopanic_url(element)

                if not title or not cryptopanic_url:
                    return {
                        "success": False,
                        "data_source": None,
                        "error": "Title or URL not found",
                    }

                # å¤–éƒ¨URLæŠ½å‡ºã‚’è©¦è¡Œ
                final_url = self._extract_external_url_safe(element, cryptopanic_url)

                # DataSourceä½œæˆ
                try:
                    published_at = self._extract_published_at(element)

                    # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
                    if not title or len(title.strip()) < 3:
                        return {
                            "success": False,
                            "data_source": None,
                            "error": f"Invalid title: '{title}'",
                        }

                    if not final_url or not (final_url.startswith("http://") or final_url.startswith("https://")):
                        return {
                            "success": False,
                            "data_source": None,
                            "error": f"Invalid URL: '{final_url}'",
                        }

                    data_source = DataSource.from_cryptopanic_news(
                        title=title.strip(),
                        url=final_url.strip(),
                        published_at=published_at,
                        scraped_at=datetime.now(UTC),
                    )
                except Exception as e:
                    return {
                        "success": False,
                        "data_source": None,
                        "error": f"DataSource creation failed: {e}",
                    }

            # ãƒ­ãƒƒã‚¯å¤–ã§æ¤œè¨¼
            if data_source and data_source.is_valid():
                return {"success": True, "data_source": data_source, "error": None}
            else:
                error_msg = "Invalid data source"
                if data_source:
                    error_msg += f" - URL: '{data_source.url}', Title: '{data_source.summary}'"
                return {
                    "success": False,
                    "data_source": None,
                    "error": error_msg,
                }

        except Exception as e:
            return {"success": False, "data_source": None, "error": str(e)}

    def _extract_title_and_url(self, element) -> tuple[str | None, str | None]:
        """è¨˜äº‹ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã¨CryptoPanic URLã‚’æŠ½å‡º"""
        try:
            # ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º
            title = self._extract_title_from_element(element)

            # CryptoPanic URLæŠ½å‡º
            cryptopanic_url = self._extract_cryptopanic_url(element)

            return title, cryptopanic_url

        except Exception as e:
            self.logger.debug(f"Failed to extract title and URL: {e}")
            return None, None

    def _extract_title_from_element(self, element) -> str | None:
        """è¦ç´ ã‹ã‚‰ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º - å …ç‰¢ç‰ˆ"""
        try:
            # Method 1: .nc-title .title-text
            title_element = ElementSearcher.safe_find_element(element, ".nc-title .title-text")
            if title_element:
                title_text = self._extract_title_text(title_element)
                if title_text:
                    return title_text

            # Method 2: .nc-titleç›´æ¥
            title_element = ElementSearcher.safe_find_element(element, ".nc-title")
            if title_element:
                title_text = self._extract_title_text(title_element)
                if title_text:
                    return title_text

            # Method 3: a[href*='/news/']ã®ãƒ†ã‚­ã‚¹ãƒˆ
            url_link = ElementSearcher.safe_find_element(element, "a[href*='/news/']")
            if url_link:
                title_text = self._extract_title_text(url_link)
                if title_text:
                    return title_text

            # Method 4: spanè¦ç´ ã‹ã‚‰æ¤œç´¢
            span_elements = ElementSearcher.safe_find_elements(element, "span")
            for span in span_elements:
                span_text = ElementSearcher.safe_get_text(span)
                if span_text and len(span_text.strip()) > 10:
                    span_class = ElementSearcher.safe_get_attribute(span, "class") or ""
                    if not TextCleaner.is_source_name_class(span_class):
                        cleaned_text = TextCleaner.extract_valid_line(span_text)
                        if cleaned_text:
                            return cleaned_text

            return None

        except Exception as e:
            self.logger.debug(f"Title extraction failed: {e}")
            return None

    def _extract_cryptopanic_url(self, element) -> str | None:
        """è¦ç´ ã‹ã‚‰CryptoPanic URLã‚’æŠ½å‡º"""
        try:
            url_link = ElementSearcher.safe_find_element(element, "a[href*='/news/']")
            if url_link:
                href = ElementSearcher.safe_get_attribute(url_link, "href")
                if href:
                    return href if href.startswith("http") else f"{self.BASE_URL}{href}"
            return None
        except Exception as e:
            self.logger.debug(f"CryptoPanic URL extraction error: {e}")
            return None

    def _extract_published_at(self, element) -> datetime | None:
        """è¨˜äº‹ã®ç™ºè¡Œæ—¥æ™‚ã‚’æŠ½å‡º"""
        try:
            # æ™‚é–“è¦ç´ ã‚’æ¢ã™
            time_elements = ElementSearcher.safe_find_elements(element, ".nc-date time, time, .time, [datetime]")

            for time_elem in time_elements:
                # datetimeå±æ€§ã‚’ç¢ºèª
                datetime_attr = ElementSearcher.safe_get_attribute(time_elem, "datetime")
                if datetime_attr:
                    try:
                        # CryptoPanicç‰¹æœ‰ã®å½¢å¼ã‚’å‡¦ç†
                        parsed_dt = self._parse_cryptopanic_datetime(datetime_attr)
                        if parsed_dt:
                            self.logger.debug(f"Parsed datetime from attribute: {parsed_dt}")
                            return parsed_dt
                    except Exception as e:
                        self.logger.debug(f"Failed to parse datetime attribute '{datetime_attr}': {e}")

                # ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç›¸å¯¾æ™‚é–“ã‚’æŠ½å‡ºãƒ»è¨ˆç®—
                time_text = ElementSearcher.safe_get_text(time_elem)
                if time_text:
                    relative_time = self._parse_relative_time(time_text)
                    if relative_time:
                        self.logger.debug(f"Parsed relative time '{time_text}' -> {relative_time}")
                        return relative_time

            # æ™‚é–“æƒ…å ±ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯Noneã‚’è¿”ã™ï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã•ã‚Œãªã„ï¼‰
            self.logger.debug("No valid time information found")
            return None

        except Exception as e:
            self.logger.debug(f"Failed to extract published_at: {e}")
            return None

    def _parse_cryptopanic_datetime(self, datetime_str: str) -> datetime | None:
        """CryptoPanicç‰¹æœ‰ã®datetimeå½¢å¼ã‚’è§£æ"""
        try:
            import re

            # å½¢å¼: "Thu Jul 24 2025 19:27:23 GMT+0000 (Coordinated Universal Time)"
            # æ­£è¦è¡¨ç¾ã§date/timeéƒ¨åˆ†ã‚’æŠ½å‡º
            pattern = r"(\w+)\s+(\w+)\s+(\d+)\s+(\d+)\s+(\d+):(\d+):(\d+)\s+GMT([+-]\d{4})"
            match = re.match(pattern, datetime_str)

            if not match:
                # dateutilã§è©¦è¡Œ
                try:
                    from dateutil import parser

                    # GMTéƒ¨åˆ†ä»¥é™ã‚’é™¤å»ã—ã¦è§£æ
                    cleaned = re.sub(r"\s+GMT.*$", "", datetime_str)
                    return parser.parse(cleaned).replace(tzinfo=UTC)
                except Exception:
                    pass
                return None

            # ãƒãƒƒãƒã—ãŸå ´åˆã€æ§‹é€ åŒ–ã—ã¦è§£æ
            day_name, month_name, day, year, hour, minute, second, tz_offset = match.groups()

            # æœˆåã‚’æ•°å€¤ã«å¤‰æ›
            month_map = {
                "Jan": 1,
                "Feb": 2,
                "Mar": 3,
                "Apr": 4,
                "May": 5,
                "Jun": 6,
                "Jul": 7,
                "Aug": 8,
                "Sep": 9,
                "Oct": 10,
                "Nov": 11,
                "Dec": 12,
            }
            month_num = month_map.get(month_name, 1)

            # datetimeã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
            dt = datetime(int(year), month_num, int(day), int(hour), int(minute), int(second), tzinfo=UTC)

            return dt

        except Exception as e:
            self.logger.debug(f"Failed to parse CryptoPanic datetime '{datetime_str}': {e}")
            return None

    def _parse_relative_time(self, time_text: str) -> datetime | None:
        """ç›¸å¯¾æ™‚é–“ãƒ†ã‚­ã‚¹ãƒˆã‚’å®Ÿéš›ã®datetimeã«å¤‰æ›"""
        try:
            import re
            from datetime import timedelta

            text = time_text.lower().strip()
            now = datetime.now(UTC)

            # åˆ†å‰ (5min, 10min ago, 30 minutes ago)
            min_match = re.search(r"(\d+)\s*(?:min|minute)s?(?:\s+ago)?", text)
            if min_match:
                minutes = int(min_match.group(1))
                return now - timedelta(minutes=minutes)

            # æ™‚é–“å‰ (2h, 5h ago, 3 hours ago)
            hour_match = re.search(r"(\d+)\s*(?:h|hour)s?(?:\s+ago)?", text)
            if hour_match:
                hours = int(hour_match.group(1))
                return now - timedelta(hours=hours)

            # æ—¥å‰ (1d, 2 days ago)
            day_match = re.search(r"(\d+)\s*(?:d|day)s?(?:\s+ago)?", text)
            if day_match:
                days = int(day_match.group(1))
                return now - timedelta(days=days)

            # "ago"ãŒå«ã¾ã‚Œã‚‹å ´åˆã¯ç›¸å¯¾æ™‚é–“ã¨ã—ã¦æ‰±ã†ãŒã€å…·ä½“çš„ãªæ•°å€¤ãŒãªã„å ´åˆã¯1æ™‚é–“å‰ã¨ã™ã‚‹
            if "ago" in text:
                self.logger.debug(f"Relative time text '{text}' detected but no specific time found, assuming 1h ago")
                return now - timedelta(hours=1)

            return None

        except Exception as e:
            self.logger.debug(f"Failed to parse relative time '{time_text}': {e}")
            return None

    def _extract_external_url_safe(self, element, fallback_url: str) -> str:
        """å®‰å…¨ãªå¤–éƒ¨URLæŠ½å‡ºï¼ˆæ”¹è‰¯ç‰ˆï¼‰"""
        try:
            # Pressè¨˜äº‹ã¯å¤–éƒ¨URLæŠ½å‡ºã‚’ã‚¹ã‚­ãƒƒãƒ—
            if WebDriverUtils.is_press_article(element):
                self.logger.debug("Press article detected, using CryptoPanic URL")
                return fallback_url

            # ã¾ãšã€ãƒ‡ãƒ¼ã‚¿å±æ€§ã‹ã‚‰ç›´æ¥å–å¾—ã‚’è©¦è¡Œ
            direct_url = self._extract_url_from_attributes(element)
            if direct_url:
                self.logger.debug(f"âœ… Direct URL found: {direct_url}")
                return direct_url

            # å¤–éƒ¨ãƒªãƒ³ã‚¯ã‚¢ã‚¤ã‚³ãƒ³ã‚’æ¢ã™ï¼ˆã‚ˆã‚Šå¤šãã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è©¦è¡Œï¼‰
            link_icon = None
            icon_selectors = [
                ".open-link-icon",
                ".si-external-link",
                ".external-link",
                "a[title*='external']",
                "a[aria-label*='external']",
                ".link-icon",
                "[data-testid*='external']",
            ]

            for selector in icon_selectors:
                try:
                    link_icon = element.find_element(By.CSS_SELECTOR, selector)
                    self.logger.debug(f"âœ… Found external link icon with selector: {selector}")
                    break
                except Exception:
                    continue

            if not link_icon:
                self.logger.debug("âŒ No external link icon found with any selector, using CryptoPanic URL")
                return fallback_url

            # ã‚¿ãƒ–æ“ä½œã§å¤–éƒ¨URLå–å¾—ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå»¶é•·ï¼‰
            external_url = self.tab_manager.click_and_get_url(link_icon, timeout=8)

            if external_url and self._is_valid_external_url(external_url):
                self.logger.debug(f"âœ… External URL extracted via tab: {external_url}")
                return external_url
            else:
                self.logger.debug(f"âŒ External URL extraction failed (url={external_url}), using CryptoPanic URL")
                return fallback_url

        except Exception as e:
            self.logger.debug(f"âŒ External URL extraction error: {e}, using fallback")
            return fallback_url

    def _extract_original_url_robust(self, element) -> str | None:
        """å¤–éƒ¨URLå–å¾— - ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆ"""
        # Pressè¨˜äº‹ã¯å¤–éƒ¨URLãªã—
        if WebDriverUtils.is_press_article(element):
            return None

        # Method 1: ãƒ‡ãƒ¼ã‚¿å±æ€§ã‹ã‚‰ç›´æ¥å–å¾—
        direct_url = self._extract_url_from_attributes(element)
        if direct_url:
            return direct_url

        # Method 2: .open-link-iconã‚’ã‚¯ãƒªãƒƒã‚¯
        icon_elements = ElementSearcher.safe_find_elements(element, ".open-link-icon")
        if icon_elements:
            return self._extract_original_url_improved(icon_elements[0])

        return None

    def _extract_url_from_attributes(self, element) -> str | None:
        """ãƒ‡ãƒ¼ã‚¿å±æ€§ã‹ã‚‰ç›´æ¥URLå–å¾— - æ”¹è‰¯ç‰ˆ"""
        try:
            # Method 1: dataå±æ€§ã‹ã‚‰å–å¾—
            data_attrs = [
                "data-url",
                "data-href",
                "data-link",
                "data-external-url",
                "data-original-url",
            ]
            for attr in data_attrs:
                url = ElementSearcher.safe_get_attribute(element, attr)
                if self._is_valid_external_url(url):
                    self.logger.debug(f"ğŸ“Š Found URL in {attr}: {url}")
                    return url

            # Method 2: ç›´æ¥çš„ãªå¤–éƒ¨ãƒªãƒ³ã‚¯
            links = ElementSearcher.safe_find_elements(element, "a[href]")
            for link in links:
                href = ElementSearcher.safe_get_attribute(link, "href")
                if self._is_valid_external_url(href):
                    self.logger.debug(f"ğŸ”— Direct external link: {href}")
                    return href

            # Method 3: click-areaè¦ç´ ï¼ˆç‰¹åˆ¥å‡¦ç†ï¼‰
            click_areas = ElementSearcher.safe_find_elements(element, "a.click-area")
            for area in click_areas:
                href = ElementSearcher.safe_get_attribute(area, "href")
                if self._is_valid_external_url(href):
                    self.logger.debug(f"ğŸ¯ Click-area external URL: {href}")
                    return href

            # Method 4: éš ã‚ŒãŸãƒªãƒ³ã‚¯è¦ç´ 
            all_links = ElementSearcher.safe_find_elements(element, "a")
            for link in all_links:
                href = ElementSearcher.safe_get_attribute(link, "href")
                # ã‚ˆã‚Šå¯›å®¹ãªæ¡ä»¶
                if href and href.startswith("http") and not href.startswith(self.BASE_URL):
                    # åºƒå‘Šã‚„ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã§ãªã„å ´åˆ
                    if not any(
                        skip in href
                        for skip in [
                            "/redirect/",
                            "javascript:",
                            "#",
                            "cryptopanic.com",
                        ]
                    ):
                        self.logger.debug(f"ğŸ” Hidden external link: {href}")
                        return href

        except Exception as e:
            self.logger.debug(f"âŒ Attribute URL extraction error: {e}")

        return None

    def _is_valid_external_url(self, url: str) -> bool:
        """æœ‰åŠ¹ãªå¤–éƒ¨URLã‹ã©ã†ã‹åˆ¤å®š - æ”¹è‰¯ç‰ˆ"""
        if not url:
            return False

        return (
            url.startswith("http")
            and not url.startswith(self.BASE_URL)
            and "/redirect/" not in url
            and not url.startswith("javascript:")
            and not url.startswith("#")
            and len(url) > 10  # æœ€å°é•·ãƒã‚§ãƒƒã‚¯
        )

    def _extract_original_url_improved(self, icon_element) -> str | None:
        """ã‚¢ã‚¤ã‚³ãƒ³ã‚¯ãƒªãƒƒã‚¯ã§URLå–å¾— - ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆ"""
        try:
            url = self.tab_manager.click_and_get_url(icon_element)

            # CryptoPanicå†…éƒ¨URLã¯é™¤å¤–
            if url and not url.startswith(self.BASE_URL):
                return url

        except Exception as e:
            self.logger.debug(f"Click extraction failed: {e}")

        return None

    def _extract_data_source(self, element) -> DataSource | None:
        """è¨˜äº‹è¦ç´ ã‹ã‚‰DataSourceã‚’æŠ½å‡º - è¨ºæ–­å¼·åŒ–ç‰ˆ"""
        try:
            # è¦ç´ ã®classæƒ…å ±ã‚’ãƒ­ã‚°å‡ºåŠ›ï¼ˆè¨ºæ–­ç”¨ï¼‰
            element_class = ElementSearcher.safe_get_attribute(element, "class") or ""
            self.logger.debug(f"Processing element with class: {element_class}")

            # ã‚¿ã‚¤ãƒˆãƒ«ã¨CryptoPanic URLã‚’å–å¾—
            title, cryptopanic_url = self._extract_title_and_url_robust(element)
            if not title:
                self.logger.debug("âŒ FAIL: No title found")
                return None

            self.logger.debug(f"âœ… Title: {title[:30]}...")
            self.logger.debug(f"âœ… CryptoPanic URL: {cryptopanic_url}")

            # Pressè¨˜äº‹ã¯å†…éƒ¨URLã®ã¿ä½¿ç”¨
            if WebDriverUtils.is_press_article(element):
                self.logger.debug("ğŸ“° Press article detected")
                if cryptopanic_url:
                    original_url = self._normalize_url(cryptopanic_url)
                    self.logger.debug(f"âœ… Using Press internal URL: {original_url}")
                else:
                    self.logger.debug("âŒ FAIL: Press article but no internal URL")
                    return None
            else:
                # å¤–éƒ¨URLå–å¾—ã‚’è©¦è¡Œ
                self.logger.debug("ğŸ”— Attempting external URL extraction")
                original_url = self._extract_original_url_robust(element)
                if original_url:
                    self.logger.debug(f"âœ… External URL found: {original_url}")
                else:
                    # å¤–éƒ¨URLå–å¾—å¤±æ•—æ™‚ã¯å†…éƒ¨URLã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    if cryptopanic_url:
                        original_url = self._normalize_url(cryptopanic_url)
                        self.logger.debug(f"âš ï¸  Using fallback internal URL: {original_url}")
                    else:
                        self.logger.debug("âŒ FAIL: No URL found (external or internal)")
                        return None

            # ãã®ä»–ã®æƒ…å ±ã‚’å–å¾—
            published_at = self._extract_published_time(element)
            currencies = self._extract_currencies(element)
            source_domain = self._extract_source_domain(element)

            # DataSourceä½œæˆ
            data_source = DataSource.from_cryptopanic_news(
                title=title.strip(),
                url=original_url,
                published_at=published_at,
                currencies=currencies,
                source_domain=source_domain,
                scraped_at=datetime.now(UTC),
            )

            if data_source.is_valid():
                self.logger.debug("âœ… DataSource validation passed")
                return data_source
            else:
                self.logger.debug("âŒ FAIL: DataSource validation failed")
                return None

        except Exception as e:
            self.logger.warning(f"Extract error: {e}")
            return None

    def _extract_title_text(self, title_element) -> str | None:
        """ã‚¿ã‚¤ãƒˆãƒ«ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º - æ”¹è‰¯ç‰ˆ"""
        try:
            # ç›´æ¥çš„ãªãƒ†ã‚­ã‚¹ãƒˆå–å¾—
            direct_text = ElementSearcher.safe_get_text(title_element)
            if direct_text and len(direct_text.strip()) > 3:
                cleaned_text = TextCleaner.extract_valid_line(direct_text)
                if cleaned_text and len(cleaned_text) > 3:
                    self.logger.debug(f"âœ… Direct title text: {cleaned_text[:30]}...")
                    return cleaned_text
                else:
                    self.logger.debug(f"âŒ Direct text failed cleaning: '{direct_text[:30]}...'")

            # spanè¦ç´ å†…ã®ãƒ†ã‚­ã‚¹ãƒˆå–å¾—
            span_elements = ElementSearcher.safe_find_elements(title_element, "span")
            self.logger.debug(f"Found {len(span_elements)} span elements")

            for i, span in enumerate(span_elements):
                span_class = ElementSearcher.safe_get_attribute(span, "class") or ""
                span_text = ElementSearcher.safe_get_text(span)

                self.logger.debug(
                    f"  Span {i + 1}: class='{span_class}', text='{span_text[:30] if span_text else None}...'"
                )

                # si-source-nameã‚¯ãƒ©ã‚¹ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒ‰ãƒ¡ã‚¤ãƒ³åãªã©ï¼‰
                if TextCleaner.is_source_name_class(span_class):
                    self.logger.debug("  Skipping source name span")
                    continue

                # ã‚ˆã‚Šå¯›å®¹ãªæ¡ä»¶
                if span_text and len(span_text.strip()) > 3:
                    cleaned_text = TextCleaner.extract_valid_line(span_text)
                    if cleaned_text and len(cleaned_text) > 3:
                        self.logger.debug(f"âœ… Span title text: {cleaned_text[:30]}...")
                        return cleaned_text

            # æœ€å¾Œã®æ‰‹æ®µï¼šã‚ˆã‚Šå¯›å®¹ãªæ¤œè¨¼ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
            all_text = ElementSearcher.safe_get_text(title_element)
            if all_text:
                # è¤‡æ•°è¡Œã«åˆ†ã‹ã‚Œã¦ã„ã‚‹å ´åˆã®å‡¦ç†
                lines = [line.strip() for line in all_text.split("\n") if line.strip()]
                for line in lines:
                    if len(line) > 5 and not self._is_likely_domain_or_source(line):
                        # é€šå¸¸ã®æ¤œè¨¼ãŒå¤±æ•—ã—ãŸå ´åˆã€ã‚ˆã‚Šå¯›å®¹ãªæ¤œè¨¼ã‚’è©¦è¡Œ
                        cleaned_text = TextCleaner.extract_valid_line(line)
                        if cleaned_text:
                            self.logger.debug(f"âœ… Fallback title text: {cleaned_text[:30]}...")
                            return cleaned_text
                        # åŸºæœ¬çš„ãªæ¤œè¨¼ã®ã¿ã§ã‚¿ã‚¤ãƒˆãƒ«ã‚’è¨±å¯
                        elif self._is_basic_valid_title(line):
                            self.logger.debug(f"âœ… Basic valid title text: {line[:30]}...")
                            return line

            self.logger.debug("âŒ No valid title text found")
            return None

        except Exception as e:
            self.logger.debug(f"âŒ Title text extraction error: {e}")
            return None

    def _is_basic_valid_title(self, text: str) -> bool:
        """åŸºæœ¬çš„ãªã‚¿ã‚¤ãƒˆãƒ«æ¤œè¨¼ï¼ˆã‚ˆã‚Šå¯›å®¹ï¼‰"""
        if not text or len(text.strip()) < 5:
            return False

        text = text.strip()

        # æ˜ã‚‰ã‹ã«ç„¡åŠ¹ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿é™¤å¤–
        if text.isdigit():
            return False

        # æ™‚é–“è¡¨ç¤ºã®ã¿ã®å ´åˆã¯é™¤å¤–
        if len(text) < 10 and any(indicator in text.lower() for indicator in ["min", "h", "ago", "hour"]):
            return False

        # ãƒ‰ãƒ¡ã‚¤ãƒ³åã®ã¿ã®å ´åˆã¯é™¤å¤–
        if len(text) < 20 and any(domain in text.lower() for domain in [".com", ".org", ".io"]):
            return False

        return True

    def _is_likely_domain_or_source(self, text: str) -> bool:
        """ãƒ‰ãƒ¡ã‚¤ãƒ³åã‚„ã‚½ãƒ¼ã‚¹åã‚‰ã—ã„ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ¤å®š"""
        text_lower = text.lower()
        # ãƒ‰ãƒ¡ã‚¤ãƒ³åã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        if "." in text and len(text) < 30:
            return True
        # ä¸€èˆ¬çš„ãªã‚½ãƒ¼ã‚¹åãƒ‘ã‚¿ãƒ¼ãƒ³
        source_indicators = [
            "coinpedia",
            "cointelegraph",
            "crypto",
            "bitcoin",
            ".com",
            ".org",
            ".io",
        ]
        return any(indicator in text_lower for indicator in source_indicators)

    def _extract_title_text_robust(self, element) -> str | None:
        """å …ç‰¢ãªã‚¿ã‚¤ãƒˆãƒ«ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º"""
        if not element:
            return None

        # Method 1: .title-textå†…ã®ãƒ¡ã‚¤ãƒ³ã‚¹ãƒ‘ãƒ³
        title_text_elements = ElementSearcher.safe_find_elements(element, ".title-text")
        if title_text_elements:
            title_spans = ElementSearcher.safe_find_elements(title_text_elements[0], "span")
            for span in title_spans:
                class_attr = ElementSearcher.safe_get_attribute(span, "class")
                text = ElementSearcher.safe_get_text(span)

                if not TextCleaner.is_source_name_class(class_attr) and TextCleaner.is_valid_title(text):
                    return text

            # ç›´æ¥ãƒ†ã‚­ã‚¹ãƒˆã‚‚è©¦è¡Œ
            direct_text = ElementSearcher.safe_get_text(title_text_elements[0])
            valid_line = TextCleaner.extract_valid_line(direct_text)
            if valid_line:
                return valid_line

        # Method 2: è¦ç´ ã®ç›´æ¥ãƒ†ã‚­ã‚¹ãƒˆï¼ˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¼·åŒ–ï¼‰
        text = ElementSearcher.safe_get_text(element)
        return TextCleaner.extract_valid_line(text)

    def _extract_published_time(self, element) -> datetime | None:
        """å…¬é–‹æ™‚åˆ»ã‚’æŠ½å‡º"""
        time_element = ElementSearcher.safe_find_element(element, ".nc-date time")
        if not time_element:
            return datetime.now(UTC)

        datetime_attr = ElementSearcher.safe_get_attribute(time_element, "datetime")
        if not datetime_attr:
            return datetime.now(UTC)

        # GMT+0900 éƒ¨åˆ†ã‚’å‰Šé™¤ã—ã¦ãƒ‘ãƒ¼ã‚¹
        cleaned = re.sub(r"\s+GMT[+-]\d{4}\s+\([^)]+\)", "", datetime_attr)
        try:
            parsed_dt = datetime.strptime(cleaned, "%a %b %d %Y %H:%M:%S")
            return parsed_dt.replace(tzinfo=UTC)
        except ValueError:
            return datetime.now(UTC)

    def _extract_currencies(self, element) -> list[str]:
        """é–¢é€£é€šè²¨ã‚’æŠ½å‡º"""
        currency_links = ElementSearcher.safe_find_elements(element, ".nc-currency a.colored-link")
        return [ElementSearcher.safe_get_text(link) for link in currency_links if ElementSearcher.safe_get_text(link)]

    def _extract_source_domain(self, element) -> str | None:
        """ã‚½ãƒ¼ã‚¹ãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’æŠ½å‡º"""
        source_element = ElementSearcher.safe_find_element(element, ".si-source-domain")
        if source_element:
            return ElementSearcher.safe_get_text(source_element)
        return None

    def _get_article_elements(self) -> list:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹è¦ç´ ã‚’å–å¾—"""
        try:
            elements = self.driver.find_elements(By.CSS_SELECTOR, ".news-row")

            # æœ‰åŠ¹ãªè¨˜äº‹ã®ã¿ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            filtered_elements = []
            for el in elements:
                if ElementSearcher.safe_find_elements(el, ".nc-title, a[href*='/news/']"):
                    filtered_elements.append(el)

            self.logger.info(f"Found {len(filtered_elements)} news elements (filtered from {len(elements)} total)")
            return filtered_elements

        except Exception as e:
            self.logger.error(f"Failed to get article elements: {e}")
            return []

    def _wait_for_page_load(self) -> None:
        """ãƒšãƒ¼ã‚¸ã®èª­ã¿è¾¼ã¿å®Œäº†ã‚’å¾…æ©Ÿ"""
        try:
            wait = WebDriverWait(self.driver.driver, 15)
            wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, ".news-row")))
            self.logger.info("CryptoPanic page loaded successfully")
            time.sleep(2)

        except TimeoutException:
            self.logger.warning("Timeout waiting for news elements, continuing anyway")
            time.sleep(3)

    def _normalize_url(self, url: str) -> str:
        """URLã‚’æ­£è¦åŒ–"""
        if url.startswith("http"):
            return url
        return urljoin(self.BASE_URL, url)
