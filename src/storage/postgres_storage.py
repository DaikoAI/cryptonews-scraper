"""
PostgreSQL Storage Implementation

data_sourceãƒ†ãƒ¼ãƒ–ãƒ«ã«ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã™ã‚‹PostgreSQLã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚¯ãƒ©ã‚¹
"""

import json
from datetime import datetime

import asyncpg

from src.models import DataSource

# BaseStorageã‚’å‰Šé™¤ã—ã€ç›´æ¥å®Ÿè£…
from src.utils.logger import get_app_logger


class PostgresStorage:
    """PostgreSQLç”¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸å®Ÿè£… - Drizzle ORMã®data_sourceãƒ†ãƒ¼ãƒ–ãƒ«å¯¾å¿œ"""

    def __init__(self, connection_string: str | None = None):
        """
        Args:
            connection_string: PostgreSQLæ¥ç¶šæ–‡å­—åˆ— (Noneã®å ´åˆã¯ç’°å¢ƒå¤‰æ•°ã‹ã‚‰å–å¾—)
        """
        from src.config import Config

        self.connection_string = connection_string or Config.DATABASE_URL
        if not self.connection_string:
            raise ValueError("DATABASE_URL environment variable is required")

        self.pool: asyncpg.Pool | None = None
        self.logger = get_app_logger(__name__)

    async def connect(self) -> None:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶šãƒ—ãƒ¼ãƒ«ã‚’ä½œæˆ"""
        if self.pool is None:
            self.logger.info("Creating PostgreSQL connection pool...")
            self.pool = await asyncpg.create_pool(self.connection_string, min_size=1, max_size=10, command_timeout=60)
            self.logger.info("PostgreSQL connection pool created successfully")

    async def save_data_sources(self, data_sources: list[DataSource]) -> int:
        """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®ãƒãƒ«ã‚¯ä¿å­˜ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰"""
        if not data_sources:
            return 0

        self.logger.info(f"ğŸ’¾ Saving {len(data_sources)} articles to database...")

        try:
            async with self.pool.acquire() as conn:
                async with conn.transaction():
                    # 1. ãƒãƒ«ã‚¯é‡è¤‡ãƒã‚§ãƒƒã‚¯
                    existing_urls = await self._get_existing_urls_bulk(conn, [ds.url for ds in data_sources])
                    existing_urls_set = set(existing_urls)

                    # 2. é‡è¤‡è¨˜äº‹ã®è©³ç´°ãƒ­ã‚°
                    duplicate_sources = [ds for ds in data_sources if ds.url in existing_urls_set]
                    if duplicate_sources:
                        self.logger.info(f"ğŸ”„ Found {len(duplicate_sources)} duplicate articles:")
                        for ds in duplicate_sources:
                            self.logger.info(f"   - Duplicate: {ds.summary[:50]}... | {ds.url}")

                    # 3. æ–°è¦ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æŠ½å‡º
                    new_data_sources = [ds for ds in data_sources if ds.url not in existing_urls_set]

                    if not new_data_sources:
                        self.logger.info("âœ¨ All articles already exist - no new data to save")
                        return 0

                    self.logger.info(f"ğŸ†• Saving {len(new_data_sources)} new articles:")
                    for ds in new_data_sources:
                        self.logger.info(f"   - New: {ds.summary[:50]}... | {ds.url}")

                    # 3. ãƒãƒ«ã‚¯insertå®Ÿè¡Œ
                    saved_count = await self._bulk_insert_data_sources(conn, new_data_sources)

                    return saved_count

        except Exception as e:
            self.logger.error(f"âŒ Bulk save operation failed: {e}")
            raise

    async def _get_existing_urls_bulk(self, conn: asyncpg.Connection, urls: list[str]) -> list[str]:
        """è¤‡æ•°URLã®æ—¢å­˜ãƒã‚§ãƒƒã‚¯ã‚’ä¸€åº¦ã«å®Ÿè¡Œ"""
        if not urls:
            return []

        # PostgreSQLã®ANYæ¼”ç®—å­ã§ä¸€åº¦ã«è¤‡æ•°URLã‚’ãƒã‚§ãƒƒã‚¯
        result = await conn.fetch("SELECT url FROM data_source WHERE url = ANY($1)", urls)
        return [row["url"] for row in result]

    async def _bulk_insert_data_sources(self, conn: asyncpg.Connection, data_sources: list[DataSource]) -> int:
        """ãƒãƒ«ã‚¯insertå®Ÿè¡Œ"""
        if not data_sources:
            return 0

        # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³: ç„¡åŠ¹ãªãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’é™¤å¤–
        valid_data_sources = []
        invalid_sources = []
        for ds in data_sources:
            if not ds.is_valid():
                invalid_sources.append(ds)
                continue
            valid_data_sources.append(ds)

        if invalid_sources:
            self.logger.warning(f"âŒ Found {len(invalid_sources)} invalid data sources:")
            for ds in invalid_sources:
                self.logger.warning(
                    f"   - Invalid: {ds.summary[:50]}... | URL: {ds.url} | Type: {ds.type} | ID: {ds.id}"
                )
        else:
            self.logger.info(f"âœ… All {len(data_sources)} data sources passed validation")

        if not valid_data_sources:
            self.logger.warning("No valid data sources to insert")
            return 0

        try:
            # ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆDataSourceã‚¯ãƒ©ã‚¹ã®å®Ÿéš›ã®å±æ€§ã‚’ä½¿ç”¨ï¼‰
            insert_data = []
            for ds in valid_data_sources:
                # published_atã®ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³æƒ…å ±ã‚’å‰Šé™¤
                published_at = ds.published_at
                if published_at and published_at.tzinfo:
                    published_at = published_at.replace(tzinfo=None)

                # raw_contentã‚’JSONæ–‡å­—åˆ—ã«å¤‰æ›
                raw_content_json = json.dumps(ds.raw_content) if ds.raw_content else None

                insert_data.append(
                    (
                        ds.id,
                        ds.type,
                        ds.url,
                        ds.summary,
                        published_at,
                        raw_content_json,
                    )
                )

            # executemanyã§ãƒãƒ«ã‚¯insert
            await conn.executemany(
                """
                INSERT INTO data_source (
                    id, type, url, summary, published_at, raw_content
                ) VALUES ($1, $2, $3, $4, $5, $6)
                """,
                insert_data,
            )

            self.logger.info(f"âœ… Bulk inserted {len(valid_data_sources)} data sources")
            self.logger.info(
                f"ğŸ“Š Save Summary: {len(valid_data_sources)} saved, {len(data_sources) - len(valid_data_sources)} failed validation"
            )
            return len(valid_data_sources)

        except Exception as e:
            self.logger.error(f"âŒ Bulk insert failed: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å€‹åˆ¥insert
            return await self._fallback_individual_insert(conn, valid_data_sources)

    async def _fallback_individual_insert(self, conn: asyncpg.Connection, data_sources: list[DataSource]) -> int:
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å€‹åˆ¥insertï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰"""
        self.logger.warning("Using fallback individual insert due to bulk insert failure")
        saved_count = 0

        for data_source in data_sources:
            try:
                await self._insert_data_source(conn, data_source)
                saved_count += 1
            except Exception as e:
                self.logger.error(f"Individual insert failed for {data_source.url}: {e}")

        return saved_count

    async def _data_source_exists_in_db(self, conn: asyncpg.Connection, url: str) -> bool:
        """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ãŒdata_sourceãƒ†ãƒ¼ãƒ–ãƒ«ã«æ—¢ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        result = await conn.fetchval("SELECT 1 FROM data_source WHERE url = $1 LIMIT 1", url)
        return result is not None

    async def _insert_data_source(self, conn: asyncpg.Connection, data_source: DataSource) -> None:
        """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’data_sourceãƒ†ãƒ¼ãƒ–ãƒ«ã«æŒ¿å…¥"""
        # Drizzleã‚¹ã‚­ãƒ¼ãƒã«åˆã‚ã›ãŸæœ€å°é™ã®insertå‡¦ç†
        # published_atã®ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³æƒ…å ±ã‚’å‰Šé™¤ï¼ˆPostgreSQLã®timestampå‹ã«åˆã‚ã›ã‚‹ï¼‰
        published_at = data_source.published_at
        if published_at and published_at.tzinfo:
            # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³æƒ…å ±ã‚’å‰Šé™¤ã—ã¦ãƒŠã‚¤ãƒ¼ãƒ–ãªdatetimeã«å¤‰æ›
            published_at = published_at.replace(tzinfo=None)

        # data_sourceãƒ†ãƒ¼ãƒ–ãƒ«ã«æŒ¿å…¥
        await conn.execute(
            """
            INSERT INTO data_source (
                id, type, url, summary, published_at, raw_content
            ) VALUES ($1, $2, $3, $4, $5, $6)
            """,
            data_source.id,  # id
            data_source.type,  # type
            data_source.url,  # url
            data_source.summary,  # summary
            published_at,  # published_at (ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ãƒŠã‚¤ãƒ¼ãƒ–)
            (json.dumps(data_source.raw_content) if data_source.raw_content else None),  # raw_content (JSONæ–‡å­—åˆ—)
            # created_atã¯çœç•¥ã—ã¦PostgreSQLã®defaultNow()ã«ä»»ã›ã‚‹
        )

        self.logger.debug(f"Inserted data source: {data_source.summary[:50]}...")

    async def get_data_sources(self, limit: int = 100, offset: int = 0) -> list[DataSource]:
        """data_sourceãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’å–å¾—"""
        if not self.pool:
            await self.connect()

        async with self.pool.acquire() as conn:
            rows = await conn.fetch(
                """
                SELECT id, type, url, summary, published_at, raw_content, created_at
                FROM data_source
                WHERE type = 'news'
                ORDER BY published_at DESC NULLS LAST, created_at DESC
                LIMIT $1 OFFSET $2
                """,
                limit,
                offset,
            )

        data_sources = []
        for row in rows:
            try:
                data_source = self._row_to_data_source(row)
                data_sources.append(data_source)
            except Exception as e:
                self.logger.warning(f"Failed to convert row to DataSource: {e}")
                continue

        return data_sources

    def _row_to_data_source(self, row) -> DataSource:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹è¡Œã‚’DataSourceã«å¤‰æ›"""
        # raw_contentã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å¾©å…ƒ
        raw_content = {}
        if row["raw_content"]:
            try:
                raw_content = json.loads(row["raw_content"])
            except json.JSONDecodeError as e:
                self.logger.warning(f"Failed to parse raw_content JSON: {e}")

        return DataSource(
            id=row["id"],
            type=row["type"],
            url=row["url"] or "",
            summary=row["summary"] or "",
            published_at=row["published_at"],
            raw_content=raw_content,
            created_at=row["created_at"],
        )

    async def data_source_exists(self, url: str) -> bool:
        """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ãŒdata_sourceãƒ†ãƒ¼ãƒ–ãƒ«ã«å­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯"""
        if not self.pool:
            await self.connect()

        async with self.pool.acquire() as conn:
            return await self._data_source_exists_in_db(conn, url)

    async def get_latest_published_at(self) -> datetime | None:
        """æœ€æ–°ã®è¨˜äº‹ã®å…¬é–‹æ—¥æ™‚ã‚’å–å¾—ï¼ˆå‰å›å–å¾—ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰"""
        if not self.pool:
            await self.connect()

        async with self.pool.acquire() as conn:
            result = await conn.fetchval(
                """
                SELECT MAX(published_at) FROM data_source
                WHERE type = 'news' AND published_at IS NOT NULL
                """
            )

            # çµæœãŒã‚ã‚‹å ´åˆã¯ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³æƒ…å ±ã‚’ç¢ºä¿
            if result and not result.tzinfo:
                from datetime import UTC

                result = result.replace(tzinfo=UTC)

            return result

    async def close(self) -> None:
        """æ¥ç¶šãƒ—ãƒ¼ãƒ«ã‚’é–‰ã˜ã‚‹"""
        if self.pool:
            self.logger.info("Closing PostgreSQL connection pool...")
            await self.pool.close()
            self.pool = None
            self.logger.info("PostgreSQL connection pool closed")


async def create_postgres_storage() -> PostgresStorage:
    """ç’°å¢ƒå¤‰æ•°ã‹ã‚‰PostgreSQLã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚’ä½œæˆ"""
    storage = PostgresStorage()
    await storage.connect()
    return storage
